{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import randn\n",
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ALL CLASSES DEFINED HERE\n",
    "\n",
    "class Sigmoid:\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.current_sigma = 1/(1+np.exp(-x))\n",
    "        return self.current_sigma\n",
    "    \n",
    "    def backward(self, gradOutput):\n",
    "        _g_v = np.multiply(np.multiply(self.current_sigma, 1-self.current_sigma), gradOutput)\n",
    "        return _g_v\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_sigma = None\n",
    "    \n",
    "    \n",
    "class Linear:\n",
    "    \n",
    "    def __init__(self, inputDim, outputDim, _var = 0.01):\n",
    "        self.weight = randn(inputDim, outputDim) * _var\n",
    "        self.bias = randn((1, outputDim)) * _var\n",
    "        \n",
    "        self.gradWeight = np.zeros_like(self.weight)\n",
    "        self.gradBias = np.zeros_like(self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        self.gradWeight = np.dot(x.T, gradOutput)\n",
    "        self.gradBias = np.dot(np.ones((1,gradOutput.shape[0])),gradOutput)\n",
    "        return np.dot(gradOutput, self.weight.T)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return (self.weight, self.bias), (self.gradWeight, self.gradBias)\n",
    "    \n",
    "    \n",
    "class MeanSquareError:\n",
    "            \n",
    "    def forward(self, predictions, labels):\n",
    "        self.current_preds = predictions\n",
    "        self.current_labels = labels\n",
    "        return np.sum(np.square(predictions - labels))\n",
    "    \n",
    "    def backward(self, predictions=0, labels=0):\n",
    "        if predictions==0:\n",
    "            predictions = self.current_preds\n",
    "            labels = self.current_labels\n",
    "            \n",
    "        _numberOfSamples = len(labels)\n",
    "        _f_u = 2*_numberOfSamples*(predictions-labels)\n",
    "        return _f_u        \n",
    "\n",
    "class SimpleModel:\n",
    "    \n",
    "    def __init__(self, customFunction):\n",
    "        print \"Creating PyDeepTensorNet Model\"\n",
    "        self.layerStack = {}\n",
    "        self.customFunction = customFunction\n",
    "        \n",
    "    def add(self, layerObject, layerName):\n",
    "        self.layerStack[layerName] = layerObject\n",
    "        print layerName,'Added'\n",
    "        \n",
    "    def run(self, inputX=None, outputY=None, \n",
    "            batchSize=1, epochs=30,\n",
    "            learningRate = 0.001,\n",
    "            verbose=True, printStep=10, shuffle=False):\n",
    "        numberOfSamples = inputX.shape[0]\n",
    "        if verbose:\n",
    "            print 'Running{0} Stochastic Gradient Descent on {1} samples'.format([' Mini-batch', ' '][batchSize==1], \n",
    "                                                                                  numberOfSamples)\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "\n",
    "            for iteration in range(numberOfSamples/batchSize):\n",
    "                if shuffle:\n",
    "                    batchRandomIndices = choice(range(numberOfSamples), batchSize)\n",
    "                    batchInput = inputX[batchRandomIndices]\n",
    "                    batchOutput = outputY[batchRandomIndices]\n",
    "                else:\n",
    "                    batchInput = inputX[iteration*(batchSize):(1+iteration)*batchSize]\n",
    "                    batchOutput = outputY[iteration*(batchSize):(1+iteration)*batchSize]\n",
    "                self.layerStack, loss = self.customFunction(self.layerStack, batchInput, batchOutput, learningRate, loss)\n",
    "            if epoch%printStep==0 and verbose:\n",
    "                print \"Epoch \",epoch,\n",
    "                print \"loss: \",loss/(batchSize*(iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1: [[ 0.4934456   0.49069751  0.49622148]]\n",
      "a2: [[ 0.50694103  0.49441347  0.47743266]]\n",
      "a: [[ 0.50801006  0.50408115  0.50736725]\n",
      " [ 0.5109484   0.50906305  0.51214163]]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([[1, 2, 2, 3]])\n",
    "a1 = Sigmoid().forward(Linear(4,3).forward(x1))\n",
    "\n",
    "x2 = np.array([[4, 5, 2, 1]])\n",
    "a2 = Sigmoid().forward(Linear(4,3).forward(x2))\n",
    "\n",
    "x = np.concatenate((x1, x2), axis=0)\n",
    "a = Sigmoid().forward(Linear(4,3).forward(x))\n",
    "\n",
    "print 'a1:',a1\n",
    "print 'a2:',a2\n",
    "print 'a:',a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6271\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([[0.23, 0.25, 0.33], [0.23, 0.25, 0.33], [0.23, 0.25, 0.33], [0.23, 0.25, 0.33]])\n",
    "y_true = np.array([[0.25, 0.25, 0.25], [0.33, 0.33, 0.33], [0.77, 0.77, 0.77], [0.80, 0.80, 0.80]])\n",
    "print MeanSquareError().forward(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = np.array([[1, 2, 2, 3]])\n",
    "y1 = np.array([[0.25, 0.25, 0.25]])\n",
    "\n",
    "linear = Linear(4,3) # Weights initialization does matter.\n",
    "sigmoid = Sigmoid()\n",
    "loss = MeanSquareError()\n",
    "\n",
    "#hacky-abstraction: loss_val = loss.forward(sigmoid.forward(linear.forward(x1)), y1)\n",
    "\n",
    "a0 = linear.forward(x1)\n",
    "a1 = sigmoid.forward(a0)\n",
    "loss_val = loss.forward(a1, y1)\n",
    "\n",
    "# Backprop\n",
    "#hacky-abstraction: dx1 = linear.backward(x1, sigmoid.backward(loss.backward()))\n",
    "\n",
    "da1 = loss.backward()\n",
    "da0 = sigmoid.backward(da1)\n",
    "dx1 = linear.backward(x1, da0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradWeight: \n",
      "[[ -2.92769612  -3.81950586  -4.91710397]\n",
      " [ -4.75437831  -6.20261636  -7.98504063]\n",
      " [-43.08968132 -56.21529139 -72.36968401]\n",
      " [ -6.66864116  -8.69998557 -11.20007014]]\n",
      "approxGradWeight: \n",
      "[[ -2.92779454  -3.81934894  -4.9168983 ]\n",
      " [ -4.75463784  -6.20220251  -7.98449823]\n",
      " [-43.11096996 -56.181266   -72.3250914 ]\n",
      " [ -6.66915171  -8.69917134 -11.199003  ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2.34, 3.8, 34.44, 5.33]])\n",
    "y = np.array([[3.2, 4.2, 5.3]])\n",
    "\n",
    "linear = Linear(4, 3)\n",
    "sigmoid = Sigmoid()\n",
    "mseLoss = MeanSquareError()\n",
    "\n",
    "#forward\n",
    "_ = mseLoss.forward(sigmoid.forward(linear.forward(x)), y)\n",
    "\n",
    "# backprop\n",
    "_ = linear.backward(x, sigmoid.backward(mseLoss.backward()))\n",
    "\n",
    "gradWeight = linear.gradWeight\n",
    "gradBias = linear.gradBias\n",
    "\n",
    "approxGradWeight = np.zeros_like(linear.weight)\n",
    "approxGradBias = np.zeros_like(linear.bias)\n",
    "\n",
    "EPSILON = 1e-4\n",
    "updatedLinear = Linear(4, 3)\n",
    "\n",
    "for i in range(linear.weight.shape[0]):\n",
    "    for j in range(linear.weight.shape[1]):\n",
    "        fw = mseLoss.forward(sigmoid.forward(linear.forward(x)), y)\n",
    "        updatedWeight = np.copy(linear.weight)\n",
    "        updatedWeight[i, j] = updatedWeight[i, j] + EPSILON\n",
    "        updatedLinear.bias = linear.bias\n",
    "        updatedLinear.weight = updatedWeight\n",
    "        fw_epsilon = mseLoss.forward(sigmoid.forward(updatedLinear.forward(x)), y) # Loss function\n",
    "        approxGradWeight[i, j] = (fw_epsilon - fw) / EPSILON\n",
    "\n",
    "# These two outputs should be similar up to some precision.\n",
    "print 'gradWeight: \\n' , gradWeight\n",
    "print 'approxGradWeight: \\n' , approxGradWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "numberOfSamples = 1000\n",
    "\n",
    "x = np.random.uniform(0, 6, (numberOfSamples, 4))\n",
    "\n",
    "# y1 = sin(x0 + x1 + x2 + x3)\n",
    "y1 = np.sin(x.sum(axis = 1))\n",
    "\n",
    "# y2 = sin(6*x1)\n",
    "y2 = np.sin(6*x[:, 1])\n",
    "\n",
    "# y3 = sin(x1 + x3)\n",
    "y3 = np.sin(x[:, 1] + x[:, 3])\n",
    "\n",
    "y = np.array([y1, y2, y3]).T\n",
    "\n",
    "print x.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(layerStack, batchInput, batchOutput, learningRate, loss):\n",
    "    \n",
    "    loss += layerStack['mse_loss'].forward(layerStack['sigmoid'].forward(layerStack['linear'].forward(batchInput)), batchOutput)\n",
    "    _ = layerStack['linear'].backward(batchInput, layerStack['sigmoid'].backward(layerStack['mse_loss'].backward()))\n",
    "\n",
    "    layerStack['linear'].weight -= layerStack['linear'].gradWeight*learningRate\n",
    "    layerStack['linear'].bias -= layerStack['linear'].gradBias*learningRate\n",
    "    \n",
    "    return layerStack, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyDeepTensorNet Model\n",
      "linear Added\n",
      "sigmoid Added\n",
      "mse_loss Added\n",
      "Running  Stochastic Gradient Descent on 1000 samples\n",
      "Epoch  0 loss:  1.61197321373\n",
      "Epoch  10 loss:  1.47270157282\n",
      "Epoch  20 loss:  1.46629027538\n",
      "Epoch  30 loss:  1.45896955882\n",
      "Epoch  40 loss:  1.44927455945\n",
      "Epoch  50 loss:  1.44184369877\n",
      "Epoch  60 loss:  1.43686463925\n",
      "Epoch  70 loss:  1.43342912556\n",
      "Epoch  80 loss:  1.43091725141\n",
      "Epoch  90 loss:  1.42897907133\n",
      "Epoch  100 loss:  1.4274151241\n",
      "Epoch  110 loss:  1.42610629622\n",
      "Epoch  120 loss:  1.42497785987\n",
      "Epoch  130 loss:  1.42398083866\n",
      "Epoch  140 loss:  1.42308195808\n",
      "Epoch  150 loss:  1.42225797761\n",
      "Epoch  160 loss:  1.4214923412\n",
      "Epoch  170 loss:  1.42077308711\n",
      "Epoch  180 loss:  1.42009145389\n",
      "Epoch  190 loss:  1.41944089137\n",
      "Epoch  200 loss:  1.41881634758\n",
      "Epoch  210 loss:  1.41821378539\n",
      "Epoch  220 loss:  1.41762989658\n",
      "Epoch  230 loss:  1.41706195805\n",
      "Epoch  240 loss:  1.41650776042\n",
      "Epoch  250 loss:  1.41596555446\n",
      "Epoch  260 loss:  1.4154339919\n",
      "Epoch  270 loss:  1.4149120595\n",
      "Epoch  280 loss:  1.4143990137\n",
      "Epoch  290 loss:  1.41389432147\n",
      "Epoch  300 loss:  1.41339760982\n",
      "Epoch  310 loss:  1.41290862394\n",
      "Epoch  320 loss:  1.41242719318\n",
      "Epoch  330 loss:  1.4119532038\n",
      "Epoch  340 loss:  1.41148657756\n",
      "Epoch  350 loss:  1.41102725548\n",
      "Epoch  360 loss:  1.4105751857\n",
      "Epoch  370 loss:  1.41013031517\n",
      "Epoch  380 loss:  1.40969258406\n",
      "Epoch  390 loss:  1.40926192262\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(trainModel)\n",
    "model.add(Linear(4,3), 'linear')\n",
    "model.add(Sigmoid(), 'sigmoid')\n",
    "model.add(MeanSquareError(), 'mse_loss')\n",
    "\n",
    "model.run(x, y, epochs=400, batchSize=1, learningRate=0.001, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Two-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNewModel(layerStack, batchInput, batchOutput, learningRate, loss):\n",
    "    linear_1_out = layerStack['linear_1'].forward(batchInput)\n",
    "    sigmoid_1_out = layerStack['sigmoid_1'].forward(linear_1_out)\n",
    "    linear_2_out = layerStack['linear_2'].forward(sigmoid_1_out)\n",
    "    sigmoid_2_out = layerStack['sigmoid_2'].forward(linear_2_out)\n",
    "    loss_out = layerStack['mse_loss'].forward(sigmoid_2_out, batchOutput)\n",
    "    \n",
    "    loss += loss_out\n",
    "    _ = layerStack['linear_1'].backward(batchInput, layerStack['sigmoid_1'].backward(layerStack['linear_2'].backward(sigmoid_1_out, layerStack['sigmoid_2'].backward(layerStack['mse_loss'].backward()))))\n",
    "    layerStack['linear_2'].weight -= layerStack['linear_2'].gradWeight*learningRate\n",
    "    layerStack['linear_2'].bias -= layerStack['linear_2'].gradBias*learningRate\n",
    "    layerStack['linear_1'].weight -= layerStack['linear_1'].gradWeight*learningRate\n",
    "    layerStack['linear_1'].bias -= layerStack['linear_1'].gradBias*learningRate\n",
    "    \n",
    "    return layerStack, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyDeepTensorNet Model\n",
      "linear_1 Added\n",
      "sigmoid_1 Added\n",
      "linear_2 Added\n",
      "sigmoid_2 Added\n",
      "mse_loss Added\n",
      "Running  Stochastic Gradient Descent on 1000 samples\n",
      "Epoch  0 loss:  1.66351357342\n",
      "Epoch  10 loss:  1.5310219077\n",
      "Epoch  20 loss:  1.53009423097\n",
      "Epoch  30 loss:  1.52926828795\n",
      "Epoch  40 loss:  1.5275236162\n",
      "Epoch  50 loss:  1.52493683009\n",
      "Epoch  60 loss:  1.52192270214\n",
      "Epoch  70 loss:  1.51856136203\n",
      "Epoch  80 loss:  1.51500364775\n",
      "Epoch  90 loss:  1.51099631982\n",
      "Epoch  100 loss:  1.50622008412\n",
      "Epoch  110 loss:  1.50114796352\n",
      "Epoch  120 loss:  1.49671717312\n",
      "Epoch  130 loss:  1.49317121684\n",
      "Epoch  140 loss:  1.49015863506\n",
      "Epoch  150 loss:  1.48716978463\n",
      "Epoch  160 loss:  1.48349514386\n",
      "Epoch  170 loss:  1.47847453381\n",
      "Epoch  180 loss:  1.47353366588\n",
      "Epoch  190 loss:  1.46947229771\n",
      "Epoch  200 loss:  1.46569469638\n",
      "Epoch  210 loss:  1.46193635497\n",
      "Epoch  220 loss:  1.45802448461\n",
      "Epoch  230 loss:  1.4538261433\n",
      "Epoch  240 loss:  1.44983177156\n",
      "Epoch  250 loss:  1.44634464506\n",
      "Epoch  260 loss:  1.4433730831\n",
      "Epoch  270 loss:  1.44085412847\n",
      "Epoch  280 loss:  1.43871291853\n",
      "Epoch  290 loss:  1.43688374487\n",
      "Epoch  300 loss:  1.43531332467\n",
      "Epoch  310 loss:  1.43395888069\n",
      "Epoch  320 loss:  1.43278566689\n",
      "Epoch  330 loss:  1.43176508647\n",
      "Epoch  340 loss:  1.43087341448\n",
      "Epoch  350 loss:  1.43009090459\n",
      "Epoch  360 loss:  1.42940110851\n",
      "Epoch  370 loss:  1.4287903202\n",
      "Epoch  380 loss:  1.42824710873\n",
      "Epoch  390 loss:  1.42776192453\n"
     ]
    }
   ],
   "source": [
    "newModel = SimpleModel(trainNewModel)\n",
    "newModel.add(Linear(4,5),'linear_1')\n",
    "newModel.add(Sigmoid(),'sigmoid_1')\n",
    "newModel.add(Linear(5,3), 'linear_2')\n",
    "newModel.add(Sigmoid(), 'sigmoid_2')\n",
    "newModel.add(MeanSquareError(), 'mse_loss')\n",
    "\n",
    "newModel.run(inputX=x, outputY=y, epochs=400, batchSize=1, learningRate=0.01) ## LR of 0.01 to make it converge faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weight Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>For x,y given in section 2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight check for W2:\n",
      "gradWeight: \n",
      "[[-0.82372958 -1.12723936 -1.46004099]\n",
      " [-0.68844974 -0.94211457 -1.2202607 ]\n",
      " [-0.66832853 -0.91457954 -1.18459634]\n",
      " [-0.66740003 -0.91330893 -1.1829506 ]\n",
      " [-0.6992864  -0.95694408 -1.23946842]]\n",
      "approxGradWeight: \n",
      "[[-0.8237277  -1.12723747 -1.46003865]\n",
      " [-0.68844843 -0.94211325 -1.22025906]\n",
      " [-0.66832729 -0.9145783  -1.1845948 ]\n",
      " [-0.6673988  -0.91330769 -1.18294906]\n",
      " [-0.69928505 -0.95694272 -1.23946673]]\n",
      "Weight check for W1:\n",
      "gradWeight: \n",
      "[[ 0.01836199 -0.01218004 -0.04033382 -0.01765361  0.03599819]\n",
      " [ 0.02981861 -0.01977955 -0.06549936 -0.02866825  0.05845859]\n",
      " [ 0.27025078 -0.17926521 -0.59363103 -0.25982488  0.52981946]\n",
      " [ 0.04182453 -0.02774342 -0.09187147 -0.04021099  0.08199587]]\n",
      "approxGradWeight: \n",
      "[[ 0.01836152 -0.01218002 -0.04033387 -0.01765363  0.03599805]\n",
      " [ 0.02981739 -0.01977949 -0.06549951 -0.02866832  0.05845823]\n",
      " [ 0.27014992 -0.17925976 -0.59364326 -0.25983068  0.52978902]\n",
      " [ 0.04182211 -0.0277433  -0.09187177 -0.04021114  0.08199515]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2.34, 3.8, 34.44, 5.33]])\n",
    "y = np.array([[3.2, 4.2, 5.3]])\n",
    "\n",
    "linear_1 = Linear(4, 5)\n",
    "sigmoid_1 = Sigmoid()\n",
    "linear_2 = Linear(5, 3)\n",
    "sigmoid_2 = Sigmoid()\n",
    "mseLoss = MeanSquareError()\n",
    "\n",
    "#forward\n",
    "linear_1_out = linear_1.forward(x)\n",
    "sigmoid_1_out = sigmoid_1.forward(linear_1_out)\n",
    "linear_2_out = linear_2.forward(sigmoid_1_out)\n",
    "sigmoid_2_out = sigmoid_2.forward(linear_2_out)\n",
    "loss_out = mseLoss.forward(sigmoid_2_out, y)\n",
    "\n",
    "# backprop\n",
    "\n",
    "_ = linear_1.backward(x, sigmoid_1.backward(linear_2.backward(sigmoid_1_out, sigmoid_2.backward(mseLoss.backward()))))\n",
    "\n",
    "gradWeight = linear_2.gradWeight\n",
    "gradBias = linear_2.gradBias\n",
    "\n",
    "approxGradWeight = np.zeros_like(linear_2.weight)\n",
    "approxGradBias = np.zeros_like(linear_2.bias)\n",
    "\n",
    "EPSILON = 1e-4\n",
    "\n",
    "updatedWeight = Linear(4, 5)\n",
    "updatedLinear = Linear(5, 3)\n",
    "\n",
    "for i in range(linear_2.weight.shape[0]):\n",
    "    for j in range(linear_2.weight.shape[1]):\n",
    "        fw = mseLoss.forward(sigmoid_2.forward(linear_2.forward(sigmoid_1_out)), y)\n",
    "        updatedWeight = np.copy(linear_2.weight)\n",
    "        updatedWeight[i, j] = updatedWeight[i, j] + EPSILON\n",
    "        updatedLinear.bias = linear_2.bias\n",
    "        updatedLinear.weight = updatedWeight\n",
    "        fw_epsilon = mseLoss.forward(sigmoid_2.forward(updatedLinear.forward(sigmoid_1_out)), y) # Loss function\n",
    "        approxGradWeight[i, j] = (fw_epsilon - fw) / EPSILON\n",
    "\n",
    "print 'Weight check for W2:'\n",
    "print 'gradWeight: \\n' , gradWeight\n",
    "print 'approxGradWeight: \\n' , approxGradWeight\n",
    "\n",
    "\n",
    "gradWeight = linear_1.gradWeight\n",
    "gradBias = linear_1.gradBias\n",
    "\n",
    "approxGradWeight = np.zeros_like(linear_1.weight)\n",
    "approxGradBias = np.zeros_like(linear_1.bias)\n",
    "\n",
    "updatedLinear = Linear(4, 5)\n",
    "\n",
    "for i in range(linear_1.weight.shape[0]):\n",
    "    for j in range(linear_1.weight.shape[1]):\n",
    "        fw = mseLoss.forward(sigmoid_2.forward(linear_2.forward(sigmoid_1.forward(linear_1.forward(x)))),y)\n",
    "        updatedWeight = np.copy(linear_1.weight)\n",
    "        updatedWeight[i, j] = updatedWeight[i, j] + EPSILON\n",
    "        updatedLinear.weight = updatedWeight\n",
    "        updatedLinear.bias = linear_1.bias\n",
    "        fw_epsilon = mseLoss.forward(sigmoid_2.forward(linear_2.forward(sigmoid_1.forward(updatedLinear.forward(x)))),y)\n",
    "        approxGradWeight[i, j] = (fw_epsilon - fw) / EPSILON\n",
    "\n",
    "\n",
    "print 'Weight check for W1:'\n",
    "print 'gradWeight: \\n' , gradWeight\n",
    "print 'approxGradWeight: \\n' , approxGradWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Other Activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ReLU Activation\n",
    "class ReLU:\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.current_x = x\n",
    "        return np.maximum(x,0)\n",
    "\n",
    "    def backward(self, gradOutput):\n",
    "        return np.multiply(gradOutput, self.current_x > 0)\n",
    "\n",
    "# TanH Activation\n",
    "class TanH:\n",
    "    \n",
    "    def forward(self, x):\n",
    "#        self.current_tanh = np.tanh(x)\n",
    "        self.current_tanh = np.divide(np.exp(x) - np.exp(-x), np.exp(x) + np.exp(-x))\n",
    "        return self.current_tanh\n",
    "    \n",
    "    def backward(self, gradOutput):\n",
    "        return np.multiply(gradOutput, (1.0 - np.power(self.current_tanh, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training with ReLU </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyDeepTensorNet Model\n",
      "linear Added\n",
      "sigmoid Added\n",
      "mse_loss Added\n",
      "Running  Stochastic Gradient Descent on 1000 samples\n",
      "Epoch  0 loss:  1.49054789639\n",
      "Epoch  10 loss:  1.4880998739\n",
      "Epoch  20 loss:  1.48713108316\n",
      "Epoch  30 loss:  1.48700853634\n",
      "Epoch  40 loss:  1.48698571405\n",
      "Epoch  50 loss:  1.48675250661\n",
      "Epoch  60 loss:  1.48660799887\n",
      "Epoch  70 loss:  1.4865043128\n",
      "Epoch  80 loss:  1.48649211958\n",
      "Epoch  90 loss:  1.48645122354\n",
      "Epoch  100 loss:  1.48643991898\n",
      "Epoch  110 loss:  1.4864191085\n",
      "Epoch  120 loss:  1.4864049491\n",
      "Epoch  130 loss:  1.48639495823\n",
      "Epoch  140 loss:  1.48638666542\n",
      "Epoch  150 loss:  1.48639453329\n",
      "Epoch  160 loss:  1.48631694196\n",
      "Epoch  170 loss:  1.48631583564\n",
      "Epoch  180 loss:  1.48631400556\n",
      "Epoch  190 loss:  1.48637253347\n",
      "Epoch  200 loss:  1.48637827597\n",
      "Epoch  210 loss:  1.48629963161\n",
      "Epoch  220 loss:  1.48637122694\n",
      "Epoch  230 loss:  1.48632844441\n",
      "Epoch  240 loss:  1.48636992592\n",
      "Epoch  250 loss:  1.48633369436\n",
      "Epoch  260 loss:  1.48636963005\n",
      "Epoch  270 loss:  1.48635309277\n",
      "Epoch  280 loss:  1.48636567618\n",
      "Epoch  290 loss:  1.48633150617\n",
      "Epoch  300 loss:  1.48633631587\n",
      "Epoch  310 loss:  1.48639810896\n",
      "Epoch  320 loss:  1.48632756663\n",
      "Epoch  330 loss:  1.48633330275\n",
      "Epoch  340 loss:  1.48633808074\n",
      "Epoch  350 loss:  1.48639512146\n",
      "Epoch  360 loss:  1.48635762273\n",
      "Epoch  370 loss:  1.48637126956\n",
      "Epoch  380 loss:  1.48644350799\n",
      "Epoch  390 loss:  1.48636665836\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(trainModel)\n",
    "model.add(Linear(4,3), 'linear')\n",
    "model.add(ReLU(), 'sigmoid') ## Implementing ReLU,, but keeping the name 'sigmoid'\n",
    "model.add(MeanSquareError(), 'mse_loss')\n",
    "\n",
    "model.run(x, y, epochs=400, batchSize=1, learningRate=0.001, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training with TanH</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyDeepTensorNet Model\n",
      "linear Added\n",
      "sigmoid Added\n",
      "mse_loss Added\n",
      "Running  Stochastic Gradient Descent on 1000 samples\n",
      "Epoch  0 loss:  1.55934265573\n",
      "Epoch  10 loss:  1.55835561261\n",
      "Epoch  20 loss:  1.55826034117\n",
      "Epoch  30 loss:  1.55824631474\n",
      "Epoch  40 loss:  1.55824336879\n",
      "Epoch  50 loss:  1.55824261156\n",
      "Epoch  60 loss:  1.55824240139\n",
      "Epoch  70 loss:  1.55824234161\n",
      "Epoch  80 loss:  1.55824232447\n",
      "Epoch  90 loss:  1.55824231955\n",
      "Epoch  100 loss:  1.55824231813\n",
      "Epoch  110 loss:  1.55824231772\n",
      "Epoch  120 loss:  1.55824231761\n",
      "Epoch  130 loss:  1.55824231757\n",
      "Epoch  140 loss:  1.55824231756\n",
      "Epoch  150 loss:  1.55824231756\n",
      "Epoch  160 loss:  1.55824231756\n",
      "Epoch  170 loss:  1.55824231756\n",
      "Epoch  180 loss:  1.55824231756\n",
      "Epoch  190 loss:  1.55824231756\n",
      "Epoch  200 loss:  1.55824231756\n",
      "Epoch  210 loss:  1.55824231756\n",
      "Epoch  220 loss:  1.55824231756\n",
      "Epoch  230 loss:  1.55824231756\n",
      "Epoch  240 loss:  1.55824231756\n",
      "Epoch  250 loss:  1.55824231756\n",
      "Epoch  260 loss:  1.55824231756\n",
      "Epoch  270 loss:  1.55824231756\n",
      "Epoch  280 loss:  1.55824231756\n",
      "Epoch  290 loss:  1.55824231756\n",
      "Epoch  300 loss:  1.55824231756\n",
      "Epoch  310 loss:  1.55824231756\n",
      "Epoch  320 loss:  1.55824231756\n",
      "Epoch  330 loss:  1.55824231756\n",
      "Epoch  340 loss:  1.55824231756\n",
      "Epoch  350 loss:  1.55824231756\n",
      "Epoch  360 loss:  1.55824231756\n",
      "Epoch  370 loss:  1.55824231756\n",
      "Epoch  380 loss:  1.55824231756\n",
      "Epoch  390 loss:  1.55824231756\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(trainModel)\n",
    "model.add(Linear(4,3), 'linear')\n",
    "model.add(TanH(), 'sigmoid') ## Implementing TanH,, but keeping the name 'sigmoid'\n",
    "model.add(MeanSquareError(), 'mse_loss')\n",
    "\n",
    "model.run(x, y, epochs=400, batchSize=1, learningRate=0.001, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comments:</b> Stuck at a minima after the 140<sup>th</sup> epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Other Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AbsoluteError:\n",
    "    \n",
    "    def forward(self, predictions, labels):\n",
    "        self.current_preds = predictions\n",
    "        self.current_labels = labels\n",
    "        return np.mean(np.abs(predictions - labels))\n",
    "\n",
    "    def backward(self):\n",
    "        # E = abs(a-y)\n",
    "        # dE/da = {+1, if a-y> 0, -1 if a-y < 0, nan at 0}\n",
    "        diff = np.array(self.current_preds-self.current_labels)\n",
    "        diff[diff>0]=1\n",
    "        diff[diff<0]=-1\n",
    "        return diff\n",
    "    \n",
    "class BinaryCrossEntropy:\n",
    "\n",
    "    def forward(self, predictions, labels):\n",
    "        self.current_preds = predictions\n",
    "        self.current_labels = labels\n",
    "        self.num_of_samples = labels.shape[0]\n",
    "        return -(labels*np.log(predictions) + (1-labels)*np.log(1-predictions)).sum()/self.num_of_samples\n",
    "    \n",
    "    def backward(self):\n",
    "        # E = -[y.log(a) + (1-y).log(1-a)]\n",
    "        # dE/da = -[y/a - (1-y)/(1-a)]\n",
    "        #       = (a-y)/a(1-a)\n",
    "        _f_u = self.num_of_samples * np.divide(self.predictions-self.labels, self.predictions*(1-self.predictions))\n",
    "        return _f_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training using Abs Loss function </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyDeepTensorNet Model\n",
      "linear_1 Added\n",
      "sigmoid_1 Added\n",
      "linear_2 Added\n",
      "sigmoid_2 Added\n",
      "mse_loss Added\n",
      "Running Mini-batch Stochastic Gradient Descent on 1000 samples\n",
      "Epoch  0 loss:  0.0708663729028\n",
      "Epoch  10 loss:  0.0645492414372\n",
      "Epoch  20 loss:  0.063837252905\n",
      "Epoch  30 loss:  0.0636777959522\n",
      "Epoch  40 loss:  0.0636156816463\n",
      "Epoch  50 loss:  0.0635799829052\n",
      "Epoch  60 loss:  0.0635599868023\n",
      "Epoch  70 loss:  0.0635458183301\n",
      "Epoch  80 loss:  0.0635352080402\n",
      "Epoch  90 loss:  0.0635270564447\n",
      "Epoch  100 loss:  0.0635207492342\n",
      "Epoch  110 loss:  0.0635159228462\n",
      "Epoch  120 loss:  0.0635117959841\n",
      "Epoch  130 loss:  0.0635082497265\n",
      "Epoch  140 loss:  0.0635052861077\n",
      "Epoch  150 loss:  0.063502686206\n",
      "Epoch  160 loss:  0.0635004237996\n",
      "Epoch  170 loss:  0.0634984752512\n",
      "Epoch  180 loss:  0.0634967354473\n",
      "Epoch  190 loss:  0.0634952688075\n",
      "Epoch  200 loss:  0.0634939880562\n",
      "Epoch  210 loss:  0.0634928219829\n",
      "Epoch  220 loss:  0.0634917286177\n",
      "Epoch  230 loss:  0.0634906993648\n",
      "Epoch  240 loss:  0.0634897267842\n",
      "Epoch  250 loss:  0.0634888043933\n",
      "Epoch  260 loss:  0.0634879265077\n",
      "Epoch  270 loss:  0.0634870881103\n",
      "Epoch  280 loss:  0.0634863018197\n",
      "Epoch  290 loss:  0.0634855568351\n",
      "Epoch  300 loss:  0.0634848365223\n",
      "Epoch  310 loss:  0.0634841376785\n",
      "Epoch  320 loss:  0.0634834588007\n",
      "Epoch  330 loss:  0.063482816362\n",
      "Epoch  340 loss:  0.0634821853249\n",
      "Epoch  350 loss:  0.0634815638162\n",
      "Epoch  360 loss:  0.0634809495268\n",
      "Epoch  370 loss:  0.0634803404478\n",
      "Epoch  380 loss:  0.0634797099671\n",
      "Epoch  390 loss:  0.063479130604\n"
     ]
    }
   ],
   "source": [
    "newModel = SimpleModel(trainNewModel)\n",
    "newModel.add(Linear(4,5),'linear_1')\n",
    "newModel.add(Sigmoid(),'sigmoid_1')\n",
    "newModel.add(Linear(5,3), 'linear_2')\n",
    "newModel.add(Sigmoid(), 'sigmoid_2')\n",
    "newModel.add(AbsoluteError(), 'mse_loss') # Using Abs error, naming it mse_loss\n",
    "\n",
    "newModel.run(inputX=x, outputY=y, epochs=400, batchSize=10, learningRate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optional:  Implement Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Layer model, trained in batches of size 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PyDeepTensorNet Model\n",
      "linear_1 Added\n",
      "sigmoid_1 Added\n",
      "linear_2 Added\n",
      "sigmoid_2 Added\n",
      "mse_loss Added\n",
      "Running Mini-batch Stochastic Gradient Descent on 1000 samples\n",
      "Epoch  0 loss:  1.63068731843\n",
      "Epoch  10 loss:  1.48897432407\n",
      "Epoch  20 loss:  1.48825396977\n",
      "Epoch  30 loss:  1.48800106844\n",
      "Epoch  40 loss:  1.48785425671\n",
      "Epoch  50 loss:  1.48774060207\n",
      "Epoch  60 loss:  1.48763066566\n",
      "Epoch  70 loss:  1.48750077955\n",
      "Epoch  80 loss:  1.48731269671\n",
      "Epoch  90 loss:  1.48696574464\n",
      "Epoch  100 loss:  1.48609689547\n",
      "Epoch  110 loss:  1.48380015017\n",
      "Epoch  120 loss:  1.47970468194\n",
      "Epoch  130 loss:  1.47365082915\n",
      "Epoch  140 loss:  1.46662785471\n",
      "Epoch  150 loss:  1.46046235155\n",
      "Epoch  160 loss:  1.45584529642\n",
      "Epoch  170 loss:  1.45245722439\n",
      "Epoch  180 loss:  1.44985073664\n",
      "Epoch  190 loss:  1.44770304345\n",
      "Epoch  200 loss:  1.44580957711\n",
      "Epoch  210 loss:  1.44404715311\n",
      "Epoch  220 loss:  1.44234869508\n",
      "Epoch  230 loss:  1.44068875442\n",
      "Epoch  240 loss:  1.43907301747\n",
      "Epoch  250 loss:  1.43752727877\n",
      "Epoch  260 loss:  1.43608460282\n",
      "Epoch  270 loss:  1.43477195901\n",
      "Epoch  280 loss:  1.43360136743\n",
      "Epoch  290 loss:  1.43256983062\n",
      "Epoch  300 loss:  1.43166531937\n",
      "Epoch  310 loss:  1.43087293115\n",
      "Epoch  320 loss:  1.43017829808\n",
      "Epoch  330 loss:  1.42956860021\n",
      "Epoch  340 loss:  1.42903254286\n",
      "Epoch  350 loss:  1.42856020907\n",
      "Epoch  360 loss:  1.42814302372\n",
      "Epoch  370 loss:  1.42777373195\n",
      "Epoch  380 loss:  1.42744628419\n",
      "Epoch  390 loss:  1.42715561863\n"
     ]
    }
   ],
   "source": [
    "newModel.run(inputX=x, outputY=y, epochs=400, batchSize=10, learningRate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example single-layer model, trained in batches of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Mini-batch Stochastic Gradient Descent on 1000 samples\n",
      "Epoch  0 loss:  1.40991962388\n",
      "Epoch  10 loss:  1.40613441236\n",
      "Epoch  20 loss:  1.4027722298\n",
      "Epoch  30 loss:  1.39984658997\n",
      "Epoch  40 loss:  1.39728410455\n",
      "Epoch  50 loss:  1.39503306352\n",
      "Epoch  60 loss:  1.39305256118\n",
      "Epoch  70 loss:  1.39130704313\n",
      "Epoch  80 loss:  1.3897647086\n",
      "Epoch  90 loss:  1.38839730088\n",
      "Epoch  100 loss:  1.38718012932\n",
      "Epoch  110 loss:  1.38609195369\n",
      "Epoch  120 loss:  1.38511470407\n",
      "Epoch  130 loss:  1.38423310761\n",
      "Epoch  140 loss:  1.38343429221\n",
      "Epoch  150 loss:  1.3827074113\n",
      "Epoch  160 loss:  1.38204331062\n",
      "Epoch  170 loss:  1.38143424281\n",
      "Epoch  180 loss:  1.38087362841\n",
      "Epoch  190 loss:  1.38035585801\n",
      "Epoch  200 loss:  1.37987612946\n",
      "Epoch  210 loss:  1.37943031448\n",
      "Epoch  220 loss:  1.37901484919\n",
      "Epoch  230 loss:  1.37862664427\n",
      "Epoch  240 loss:  1.37826301106\n",
      "Epoch  250 loss:  1.37792160043\n",
      "Epoch  260 loss:  1.37760035227\n",
      "Epoch  270 loss:  1.37729745339\n",
      "Epoch  280 loss:  1.37701130242\n",
      "Epoch  290 loss:  1.37674048036\n",
      "Epoch  300 loss:  1.37648372577\n",
      "Epoch  310 loss:  1.37623991388\n",
      "Epoch  320 loss:  1.37600803881\n",
      "Epoch  330 loss:  1.37578719845\n",
      "Epoch  340 loss:  1.37557658157\n",
      "Epoch  350 loss:  1.3753754567\n",
      "Epoch  360 loss:  1.37518316268\n",
      "Epoch  370 loss:  1.37499910038\n",
      "Epoch  380 loss:  1.37482272563\n",
      "Epoch  390 loss:  1.37465354303\n"
     ]
    }
   ],
   "source": [
    "model.run(x, y, epochs=400, batchSize=10, learningRate=0.001, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Comments:</b> It can be seen that using batches speeds up the convergence of the training.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out random-sampling for training\n",
    "### Single Sample at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  Stochastic Gradient Descent on 1000 samples\n",
      "Epoch  0 loss:  1.38761800322\n",
      "Epoch  10 loss:  1.42942156931\n",
      "Epoch  20 loss:  1.4272957911\n",
      "Epoch  30 loss:  1.38622805864\n",
      "Epoch  40 loss:  1.41415761276\n",
      "Epoch  50 loss:  1.38676261014\n",
      "Epoch  60 loss:  1.41321475101\n",
      "Epoch  70 loss:  1.39395877606\n",
      "Epoch  80 loss:  1.39836963994\n",
      "Epoch  90 loss:  1.40333077509\n",
      "Epoch  100 loss:  1.42287634218\n",
      "Epoch  110 loss:  1.4283698032\n",
      "Epoch  120 loss:  1.39631036789\n",
      "Epoch  130 loss:  1.42875055558\n",
      "Epoch  140 loss:  1.39602671236\n",
      "Epoch  150 loss:  1.38502227728\n",
      "Epoch  160 loss:  1.39991312008\n",
      "Epoch  170 loss:  1.4069977729\n",
      "Epoch  180 loss:  1.40141082978\n",
      "Epoch  190 loss:  1.39612246034\n",
      "Epoch  200 loss:  1.40856251505\n",
      "Epoch  210 loss:  1.41626785692\n",
      "Epoch  220 loss:  1.36902450558\n",
      "Epoch  230 loss:  1.44089866394\n",
      "Epoch  240 loss:  1.39521069621\n",
      "Epoch  250 loss:  1.41724248797\n",
      "Epoch  260 loss:  1.36194033613\n",
      "Epoch  270 loss:  1.42208954534\n",
      "Epoch  280 loss:  1.41621584672\n",
      "Epoch  290 loss:  1.39080190325\n",
      "Epoch  300 loss:  1.38201960602\n",
      "Epoch  310 loss:  1.41418542272\n",
      "Epoch  320 loss:  1.35812209171\n",
      "Epoch  330 loss:  1.39636942622\n",
      "Epoch  340 loss:  1.38586070486\n",
      "Epoch  350 loss:  1.40904319968\n",
      "Epoch  360 loss:  1.40890024792\n",
      "Epoch  370 loss:  1.39358924202\n",
      "Epoch  380 loss:  1.37884955027\n",
      "Epoch  390 loss:  1.38767124811\n"
     ]
    }
   ],
   "source": [
    "model.run(x, y, epochs=400, batchSize=1, learningRate=0.001, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking 10 random samples at a time (random batch of size 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Mini-batch Stochastic Gradient Descent on 1000 samples\n",
      "Epoch  0 loss:  1.40725059097\n",
      "Epoch  10 loss:  1.41112266304\n",
      "Epoch  20 loss:  1.38854570903\n",
      "Epoch  30 loss:  1.37914962743\n",
      "Epoch  40 loss:  1.40715187718\n",
      "Epoch  50 loss:  1.37637128226\n",
      "Epoch  60 loss:  1.40719156427\n",
      "Epoch  70 loss:  1.4101033901\n",
      "Epoch  80 loss:  1.39226346443\n",
      "Epoch  90 loss:  1.40471213107\n",
      "Epoch  100 loss:  1.3766723526\n",
      "Epoch  110 loss:  1.40190021814\n",
      "Epoch  120 loss:  1.38769168943\n",
      "Epoch  130 loss:  1.35716558207\n",
      "Epoch  140 loss:  1.36816255405\n",
      "Epoch  150 loss:  1.37079817963\n",
      "Epoch  160 loss:  1.35048676313\n",
      "Epoch  170 loss:  1.42834480066\n",
      "Epoch  180 loss:  1.3738737437\n",
      "Epoch  190 loss:  1.34209541294\n",
      "Epoch  200 loss:  1.37617470578\n",
      "Epoch  210 loss:  1.37406859883\n",
      "Epoch  220 loss:  1.35919854812\n",
      "Epoch  230 loss:  1.32920991944\n",
      "Epoch  240 loss:  1.3821722894\n",
      "Epoch  250 loss:  1.36661350841\n",
      "Epoch  260 loss:  1.36029899663\n",
      "Epoch  270 loss:  1.34078426684\n",
      "Epoch  280 loss:  1.3852981694\n",
      "Epoch  290 loss:  1.39637509171\n",
      "Epoch  300 loss:  1.41198779102\n",
      "Epoch  310 loss:  1.36108668833\n",
      "Epoch  320 loss:  1.37787207419\n",
      "Epoch  330 loss:  1.42440567271\n",
      "Epoch  340 loss:  1.40691540566\n",
      "Epoch  350 loss:  1.3573455724\n",
      "Epoch  360 loss:  1.33488789044\n",
      "Epoch  370 loss:  1.36563009632\n",
      "Epoch  380 loss:  1.3807826663\n",
      "Epoch  390 loss:  1.38096287114\n"
     ]
    }
   ],
   "source": [
    "model.run(x, y, epochs=400, batchSize=10, learningRate=0.001, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:</b> The training convergence using Random sampling is not really monotonous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking for Batch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight check for W2:\n",
      "\n",
      "gradWeight: \n",
      "[[ 0.05271095  0.36303113  0.08622958]\n",
      " [ 0.02351996  0.2878376   0.36617409]\n",
      " [ 0.03457675  0.30030316  0.27429531]\n",
      " [ 0.02623346  0.29003086  0.34441089]\n",
      " [ 0.08507576  1.23521372 -0.05303475]]\n",
      "approxGradWeight: \n",
      "[[ 0.05271167  0.36312971  0.08633201]\n",
      " [ 0.02351967  0.28811941  0.36624498]\n",
      " [ 0.03457664  0.30057402  0.2743752 ]\n",
      " [ 0.02623321  0.29031064  0.34448307]\n",
      " [ 0.08507863  1.2353177  -0.0527599 ]]\n",
      "\n",
      "Weight check for W1:\n",
      "\n",
      "gradWeight: \n",
      "[[-0.29137329  0.46947352  0.17792248  0.40948762 -2.20020529]\n",
      " [ 0.1260089   0.03470236  0.09857441  0.03926862 -1.37334271]\n",
      " [-0.61088509  0.34243408  0.06516932  0.28271924 -2.50210997]\n",
      " [-0.57915531  0.04761378 -0.14647811  0.01072872 -2.31327458]]\n",
      "approxGradWeight: \n",
      "[[-0.28891018  0.47329209  0.18037311  0.41293373 -2.18378859]\n",
      " [ 0.12633085  0.03556904  0.09912678  0.04005361 -1.3730511 ]\n",
      " [-0.60807715  0.34678807  0.06792259  0.28659999 -2.48648126]\n",
      " [-0.57749569  0.04862766 -0.1458042   0.01163421 -2.29905387]]\n"
     ]
    }
   ],
   "source": [
    "linear_1 = newModel.layerStack['linear_1']\n",
    "linear_2= newModel.layerStack['linear_2']\n",
    "sigmoid_1 = newModel.layerStack['sigmoid_1']\n",
    "sigmoid_2 = newModel.layerStack['sigmoid_2']\n",
    "mseLoss = newModel.layerStack['mse_loss']\n",
    "\n",
    "batchSize = 1000\n",
    "#'x' and 'y' in this case is a batch of 1000 samples\n",
    "linear_1_out = linear_1.forward(x) \n",
    "sigmoid_1_out = sigmoid_1.forward(linear_1_out)\n",
    "linear_2_out = linear_2.forward(sigmoid_1_out)\n",
    "sigmoid_2_out = sigmoid_2.forward(linear_2_out)\n",
    "loss_out = mseLoss.forward(sigmoid_2_out, y)\n",
    "\n",
    "# backprop\n",
    "\n",
    "_ = linear_1.backward(x, sigmoid_1.backward(linear_2.backward(sigmoid_1_out, sigmoid_2.backward(mseLoss.backward()))))\n",
    "\n",
    "gradWeight = linear_2.gradWeight/batchSize\n",
    "gradBias = linear_2.gradBias/batchSize\n",
    "\n",
    "approxGradWeight = np.zeros_like(linear_2.weight)\n",
    "approxGradBias = np.zeros_like(linear_2.bias)\n",
    "\n",
    "EPSILON = 1e-4\n",
    "\n",
    "updatedWeight = Linear(4, 5)\n",
    "updatedLinear = Linear(5, 3)\n",
    "\n",
    "for i in range(linear_2.weight.shape[0]):\n",
    "    for j in range(linear_2.weight.shape[1]):\n",
    "        fw = mseLoss.forward(sigmoid_2.forward(linear_2.forward(sigmoid_1_out)), y)\n",
    "        updatedWeight = np.copy(linear_2.weight)\n",
    "        updatedWeight[i, j] = updatedWeight[i, j] + EPSILON\n",
    "        updatedLinear.bias = linear_2.bias\n",
    "        updatedLinear.weight = updatedWeight\n",
    "        fw_epsilon = mseLoss.forward(sigmoid_2.forward(updatedLinear.forward(sigmoid_1_out)), y) # Loss function\n",
    "        approxGradWeight[i, j] = (fw_epsilon - fw) / EPSILON\n",
    "\n",
    "print 'Weight check for W2:\\n'\n",
    "print 'gradWeight: \\n' , gradWeight\n",
    "print 'approxGradWeight: \\n' , approxGradWeight\n",
    "\n",
    "gradWeight = linear_1.gradWeight/batchSize\n",
    "gradBias = linear_1.gradBias/batchSize\n",
    "\n",
    "approxGradWeight = np.zeros_like(linear_1.weight)\n",
    "approxGradBias = np.zeros_like(linear_1.bias)\n",
    "\n",
    "updatedLinear = Linear(4, 5)\n",
    "\n",
    "for i in range(linear_1.weight.shape[0]):\n",
    "    for j in range(linear_1.weight.shape[1]):\n",
    "        fw = mseLoss.forward(sigmoid_2.forward(linear_2.forward(sigmoid_1.forward(linear_1.forward(x)))),y)\n",
    "        updatedWeight = np.copy(linear_1.weight)\n",
    "        updatedWeight[i, j] = updatedWeight[i, j] + EPSILON\n",
    "        updatedLinear.weight = updatedWeight\n",
    "        updatedLinear.bias = linear_1.bias\n",
    "        fw_epsilon = mseLoss.forward(sigmoid_2.forward(linear_2.forward(sigmoid_1.forward(updatedLinear.forward(x)))),y)\n",
    "        approxGradWeight[i, j] = (fw_epsilon - fw) / EPSILON\n",
    "\n",
    "\n",
    "print '\\nWeight check for W1:\\n'\n",
    "print 'gradWeight: \\n' , gradWeight\n",
    "print 'approxGradWeight: \\n' , approxGradWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The weights are indeed correctly updated even for batch implementation</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
